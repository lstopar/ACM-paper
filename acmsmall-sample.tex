% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{verbatim}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

\graphicspath{{img/}}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator{\logit}{logit}

% Metadata Information
\acmVolume{9}
\acmNumber{4}
\acmArticle{39}
\acmYear{2010}
\acmMonth{3}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\doi{0000001.0000001}

%ISSN
\issn{1234-56789}

% Document starts
\begin{document}

% Page heads
\markboth{G. Zhou et al.}{A Multifrequency MAC Specially Designed for WSN Applications}

% Title portion
\title{A Multi-Level Methodology for Explaining Data Streams}
\author{
LUKA STOPAR
\affil{Jozef Stefan Institute}
PRIMOZ SKRABA
\affil{Jozef Stefan Institute}
DUNJA MLADENIC
\affil{Jozef Stefan Institute}
MARKO GROBELNIK
\affil{Jozef Stefan Institute}
}
% NOTE! Affiliations placed here should be for the institution where the
%       BULK of the research was done. If the author has gone to a new
%       institution, before publication, the (above) affiliation should NOT be changed.
%       The authors 'current' address may be given in the "Author's addresses:" block (below).
%       So for example, Mr. Abdelzaher, the bulk of the research was done at UIUC, and he is
%       currently affiliated with NASA.

\begin{abstract}
This paper presents a novel multi-scale methodology for modeling a collection of continuously time-varying
data streams. The data streams are aggregated using unsupervised data mining methods. Typical system states
are then computed on the aggregated data. This is used as input to construct a Markovian transition model
capturing the dynamics of the monitored system. The hierarchical organization of the states enables a
visual representation of the dynamics on multiple aggregation levels.
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%
% End generated code
%

% We no longer use \terms command
%\terms{Design, Algorithms, Performance}

\keywords{\textcolor{red}{[TODO]}}

\acmformat{Luka Stopar, Primoz Skraba, Dunja Mladenic, Marko Grobelnik, 2015. A Multi-Level 
Methodology for Explaining Data Streams.}
% At a minimum you need to supply the author names, year and a title.
% IMPORTANT:
% Full first names whenever they are known, surname last, followed by a period.
% In the case of two authors, 'and' is placed between them.
% In the case of three or more authors, the serial comma is used, that is, all author names
% except the last one but including the penultimate author's name are followed by a comma,
% and then 'and' is placed before the final author's name.
% If only first and middle initials are known, then each initial
% is followed by a period and they are separated by a space.
% The remaining information (journal title, volume, article number, date, etc.) is 'auto-generated'.

\begin{bottomstuff}
\textcolor{red}{[TODO]}
This work is supported by the National Science Foundation, under
grant CNS-0435060, grant CCR-0325197 and grant EN-CS-0329609.

Author's addresses: G. Zhou, Computer Science Department,
College of William and Mary; Y. Wu  {and} J. A. Stankovic,
Computer Science Department, University of Virginia; T. Yan,
Eaton Innovation Center; T. He, Computer Science Department,
University of Minnesota; C. Huang, Google; T. F. Abdelzaher,
(Current address) NASA Ames Research Center, Moffett Field, California 94035.
\end{bottomstuff}

\maketitle


\section{Introduction}
\label{sec:introduction}

Sensory systems typically operate in cycles with a continuously time-varying. These 
systems include, for example, the solar system, manufacturing systems or weather 
systems. Such systems can be characterized by a set of states, along with associated
state transitions. States, on a high level, may include a "day" state and a "night"
state or maybe states with high and low productivity. For example, when a pilot wishes
to change an aircrafts heading, they will put the aircraft into state "banking turn"
by lowering one aileron and raising the other, causing the aircraft to perform a
circular arc. After some time, the wings of the aircraft will be brought level by
an opposing motion of the ailerons and the aircraft will go back into state "level".

Such high-level states can be decomposed into lower-level states, giving us a 
multi-level view of the system and allowing us to observe the system on multiple 
aggregation levels. For example, a "banking turn" state can be decomposed by the
aircrafts roll and angular velocity, resulting in perhaps three states: "initiate
turn", "full turn" and "end turn".

We present a methodology for modeling such systems and demonstrate its implementation
called StreamStory. StreamStory models such systems as a hierarchical Markovian process
by automatically learning the typical low-level states and transitions, and aggregating 
them into a hierarchy of Markovian processes. As such it gives users a unique view into
the monitored system.

Furthermore, we divide the inputs streams into two sets: observation and control set.
Attributes in the observation set are the attributes that tell us the state of the 
system and, we assume, cannot directly influence its dynamics. These are parameters
that users cannot directly manipulate, like aircraft tilt from the previous example,
which must be indirectly manipulated through the angles of ailerons. We use observation
attributes to identify, and aggregate, low-level states, detect outliers (anomalies) 
and determine the current state of the system.

In contrast, users can directly manipulate attributes in the control set. These 
are attributes like angles of ailerons that may directly influence the behavior 
(observation attributes) and performance of the system. For example, when an operator 
in a steel factory sets the cooling temperature to a high value, the product will
take longer to go from state "hot" to state "cool". As such, we assume, control attributes
may influence the occurrence, and expected time, of undesired states, associated with
undesired events.

Our system uses control attributes to model state transitions, allowing us to observe
the dynamics with respect to the current configuration and gives us insight into the
expected dynamics with respect to some alternate attribute configuration.

We implement our methodology in a four step process, which includes:
\begin{enumerate}
	\label{label:procedure-high-level}
	\item \label{label:procedure-preprocessing} aggregating and resampling the input
	streams, interpolating wherever needed, producing feature vectors,
	\item clustering the feature vectors to obtain lowest-level states and computing 
	statistics on each state,
	\item aggregating lowest-level states into a state hierarchy and computing the level
	on which each aggregated state lives and
	\item modeling state transitions with respect to control attributes and computing
	statistics on transitions.
\end{enumerate}
\textcolor{red}{[TODO The remainder of this paper is structured as follows]}

\section{Proposed Multi-Level Methodology}

To implement our multi-level approach, we propose a three step methodology shown
in figure \ref{fig:methodology}.

\begin{figure}[h!]
	\centering
	\makebox[\textwidth]{\includegraphics[width=\textwidth]{methodology}}
	\caption{The proposed multi-level methodology.}
	\label{fig:methodology}
\end{figure}

As shown in the figure, we split our methodology into three main steps: merging and
processing data streams, state aggregation and transition modeling. These will be 
explained in detail in the following subsections.

\subsection{Merging and Processing Data Streams}

In recent years much attention of the research community has been focused on a
class of applications where the data are modeled best not as persistent relations,
but rather as transient data streams \cite{Babcock:2002:MID:543613.543615}. Data 
streams differ from conventional batch data in several ways:
\begin{enumerate}
	\item the data elements arrive online, sequentially and the system has no
	control over their order,
	\item data streams are potentially unbounded.
\end{enumerate}
As such a data stream can be defined as a sequence of pairs $(t_i,x_i)_{i \ge 0}$
where $t_i$ is the timestamp of the $i$-th example and $x_i$ is its value.
In the first step our methodology consumes batches of multiple such streams and
merges them to produce feature vectors, interpolating the values wherever
appropriate resulting in a [TODO].

\subsection{State Aggregation}

To be able to represent the data streams in a qualitative manner

Once the data streams are merged we construct a state hierarchy. We do this 
by constructing a dendrogram of the previously constructed feature vectors.
A dendrogram is a nested sequence of partitions with associated numerical
level and is thus ideal for our multi-level representation. The literature
proposes many algorithms for constructing dendrograms. These generally fall
into two groups \cite{Maimon:2005:DMK:1088958}: 
\begin{enumerate}
	\item \textbf{Agglomerative} - each object initially represents a cluster of its own. Clusters are
	then iteratively merged, until the desired structure is obtained.
	\item \textbf{Divisive} - when the procedure starts, all object belong to the same cluster. Then
	clusters are recursively divided into sub-clusters until the desired structure is obtained.
\end{enumerate}

In general however, the time complexity of agglomerative clustering algorithms
is $\Omega(n^2)$ \cite{Sibson01011973}, which is impractical for large datasets.
Thus we compute an initial set of partitions using k-means and use them as
singletons of the dendrogram as well as the lowest level state space in our methodology.


\begin{comment}


 We
first partition the feature vectors into a set of $k$ partitions. While partitioning is an
NP complete problem, there are several methods available which compute an approximation
efficiently. These include k-means, k-modes, CLARA, DP-means, etc. \cite{6558109,DBLP:journals/corr/abs-1111-0352}.

In our methodology we choose k-means for its simplicity [TODO].

The algorithm outputs a set of $k$ centroids $c_i$ where 

We note that when using our methodology, the number of partitions should be selected
based on domain knowledge or user experience. For instance in the example below
we chose $k = 12$, one for each month in a year.



 using clustering. 

 The advantages of cluster
partitions come from their geometric properties.
We do this by
first partitioning the feature vectors obtaining a set of $k$ partitions. The literature
proposes many partitioning algorithms.
 Clustering is an
unsupervised machine learning procedure, which organizes a collection of patterns,
usually represented
\end{comment}

\subsection{Modeling Transitions}

We model transitions using a Markovian model. The main characteristics of Markovian
models is that they retain no memory of where they have been in the past. This means
that only the current state can influence where the process will go next. In this 
work we are interested only in processes that can assume a finite set of states, 
called Markov chains \cite{norris1998markov}.
More formally, we are interested in Markov chains $(X_t)_{t \ge 0}$ which can assume
values in a finite state space $S$. We define $p_{ij}(t)$ to be the probability of
the process being in state $j$ at time $t$ when starting from state $i$ at time $0$.
We define a stochastic matrix $P(t)$, where $(P(t))_ij = p_{ij}(t)$. Thus each row
of $P(t)$ is a probability distribution over the state space. Such a chain can be 
represented by a transition rate matrix $Q$, where $Q$ satisfies the following system
of equations:
\begin{equation}
	\label{eq:q-matrix}
	\frac{d}{dt}P(t) = P(t)Q, \qquad P(0) = I.
\end{equation}
Each non-diagonal entry of $Q$ thus represents the rate of going from one state
to another. $Q$ has the following properties:

The $ij$-th entry of $Q$ then represents the rate of going from state $i$ to state $j$.

We assume that $P(t)$
is recurrent for all $t$ and define $\pi_j = \lim_{t \rightarrow \infty} p_{ij}(t)$.
It can be shown that for recurrent chains $\pi_j$ is independent of the starting state.
The vector $\pi = (\pi_1, \pi_2, ..., \pi_k)$ is called the stationary distribution
of $(X_t)_{t \ge 0}$ and represents the proportion of time the process spends in 
each state after funning for an infinite amount of time.
\begin{enumerate}
	\item $-\infty < q_{ii} \le 0$
	\item $q_{ij} \ge 0$ for $j \neq i$
	\item $\sum_{i \in S}q_{ij} = 0$
\end{enumerate}
Using this representation, the stationary distribution $\pi$ can be computed as the
left eigenvector of $Q$ corresponding to eigenvalue $0$.

\subsection{Transition Rate Estimation}

We estimate the transition rates of $(X_t)_{t \ge 0}$ by first discretizing the
the continuous parameter space $t \in \left[0,\infty\right)$ into a discrete 
sequence $(0, h, 2h, ...)$ and estimating the transition probabilities
$\tilde{p}_{ij} = p_{ij}(h)$. Once $\tilde{p}_{ij}$ are estimated, the transition
intensities can be calculated as 
\begin{equation}
	q_{ij} = \frac{h}{\tilde{p}_{ij}}.
\end{equation}
As proposed in section \ref{sec:introduction} we allow the users to simulate
the dynamics based on an alternate configuration of the attributes in the 
control set $A_S$.
Lets say the process is in state $i$ at time $0$ and define a random variable
$J_i = j \Leftrightarrow X_h = j$. Thus $J_i$ has a multinomial distribution
with parameters $(p_{i1}, p_{i2}, ..., p_{in})$ which can be modeled using
a generalized linear model (GLM) \cite{glm-introduction}. Because there is
no natural ordering in the response category, we use nominal logistic regression
to estimate $p_{ij}$ based on values $x_k \in A_S$. We select reference category
as $p_{ii}$ and model the relationship between $x_k$ and $p_{ij}$ as follows:
\begin{equation}
	\logit(p_{ij}) = \log\left(\frac{p_{ij}}{p_{ii}}\right) = \beta_{ij} x_k
\end{equation}
thus, for each state we get a set of $n-1$ equations which are used simultaneously
to estimate the parameters $\beta_{ij}$. Once these have been obtained, the linear
predictors can be calculated as:
\begin{equation}
	\tilde{p}_{ij}(x_k) = \frac{e^{\beta_{ij}x_k}}{1 + \sum_{l \neq i}e^{\beta_{il}x_k}}.
\end{equation}


\subsection{State Aggregation}

Now that the transitions are being modeled on the lowest level, we need to model
them on each level of our hierarchical model.
[TODO]

\section{Theory}

This section presents the theory behind the StreamStory systems. We begin with
a general discussion about data streams, then we give some insight into the 
theory behind clustering algorithms and stochastic processes.

\begin{comment}
High dimensional systems arise in a variety of applications. In many cases these are
stochastic by nature or can be well approximated by stochastic processes. Other systems
that model high dimensional data include systems that model the system as a Markovian 
state model [TODO ref] and milestoning [TODO ref]. There are also several other techniques
[TODO ref] [TODO razdelaj].
\end{comment}

Data streams can be defined by a set of tuples $(t_i, x_i)$, where $t_i$ represents
the timestamp of the $i$-th observation $x_i$. They can be characterized by the following:
\begin{enumerate}
	\item the elements of the data stream arrive sequentially, and the developer
	has no control over their timing,
	\item each stream can provide elements at its own schedule: they do not need
	to have the same data rates or types
	\item a data stream is potentially unbounded in size.
\end{enumerate}
A data stream may contain sensor data, image data or maybe internet and web traffic.
High dimensional data streams arise in many applications. In many cases these
are stochastic in nature or can be approximated by a stochastic process \cite{2014arXiv1404.0667C}.

One of the major challenges when modeling such processes is the ability to reach
experimentally relevant timescales \cite{pande-beauchamp-bowman:2010:methods:markov-model-review}.

Markov state models build models with $N$ states and parametrize the model with the
rates between states. The challenges when building Markov state models include \cite{pande-beauchamp-bowman:2010:methods:markov-model-review}:
\begin{enumerate}
	\item defining the states of the model and
	\item using the state decomposition to build a transition matrix in an efficient
	manner.
\end{enumerate}

To define the models' states, we chose to partition the data streams.

\subsection{Gaussian Mixture Models and K-means}

A mixture model is a probabilistic model that represents the presents of subpopulations
in an observed population. A mixture model corresponds to a mixture distribution
that represents the distribution of observations in the overall population.
In a Gaussian mixture model, the data is assumed to arise from the following distribution:
\begin{equation}
	\label{eq:gaussian-mixture}
	p(x) = \sum_{i=1}^{k}\pi_i N(\mu_i, \Sigma_i)
\end{equation}
where $k$ is the number of components, $\pi$ is the mixing coefficient and $\mu_i$ and
$\Sigma_i$ are parameters of a Gaussian distribution. \textcolor{yellow}{[TODO]}
A related model is provided by the K-means objective function, which constructs a 
hard partitioning of the data set. Given a set of points $x_1, x_2, ..., x_n$, the 
K-means objective function attempts to find partitions $c_1, c_2, ..., c_k$ that
minimize the following objective function:

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{equation}
	\label{eq:k-means-obj}
	\min_{bounds} \sum_{j = 1}^{k}\sum_{x_i \in r(j)} \norm{x_i - \mu_j}_2^2
	where \mu_j = \frac{1}{|r(j)|}\sum_{x_i \in r(j)} x_i
\end{equation}

\subsection{What is left}

Once the model is build it can be used for quantitative simulations as well as
qualitative interpretation.

To define the models' states, we use 

% Head 1
\section{Implementation}

This section presents the detailed implementation of the StreamStory system, by 
presenting in details each step of our four step process. We begin the section
by first discussing the preprocessing step proposed in step \ref{label:procedure-preprocessing} 
of our four step process defined in section \ref{sec:introduction}.

% Head 2
\subsection{Data Stream Aggregation and Resampling}

In recent years much attention in the research community has been focused on a new
class of applications: applications in which the data is modeled best not as persistent
relations but rather as transient data stream \cite{Babcock:2002:MID:543613.543615}.

Data streams differ from conventional batch data in several ways:
\begin{list}{-}{}
	\item the data elements arrive online, sequentially and the system has no control
	over their order and
	\item data streams are potentially unbounded.
\end{list}

As such a data stream can be defined as a sequence of pairs $(t_i,x_i)_{i \ge 0}$, 
where $t_i$ represents the timestamp if the $i$-th measurement and $x_i$ represents 
its value. In our work we assume the streams follow the following model: 
$x_{i+1} = x_i + X$, where $X$ is a normally distributed random variable $X \sim N(\mu, \sigma)$.
\textcolor{red}{[TODO really???]}

To build a model in offline mode, StreamStory consumes batches of multiple such
data streams and merges them, to that all the stream are sampled at the same timestamps.
This results in a single joined data stream in the following form:

\begin{equation}
	\nonumber
	\left\{\left(t_1, x_1^{(1)}, x_1^{(2)}, ..., x_1^{(d)}\right), \left(t_1 + \Delta t, x_2^{(1)}, x_2^{(2)}, ..., x_2^{(d)}\right), ..., \left(t_0 + (k-1)\Delta t, x_k^{(1)}, x_k^{(2)}, ..., x_k^{(d)}\right)\right\}
\end{equation}

Since in general not all the input streams are equally sampled, missing values need
to be interpolated. For this purpose StreamStory supports two interpolation methods:
linear and previous point interpolation. The choice of the interpolation method
influences the online behavior of the system. Although in theory linear interpolation
produces better results \textcolor{red}{[TODO ref]}, linear interpolation cannot 
interpolate a value until it observes at least one value in the \textcolor{red}{[TODO future]}.
Which results in a certain lag (non-real-time behavior) of the system.

\subsection{State Identification}

Once the data streams are preprocessed, StreamStory identifies their typical lowest-level
states. This is achieved by clustering the joined data stream.

Clustering is an unsupervised machine learning procedure, which organizes a collection
of patterns, usually represented as a vectors of measurements or points in a 
multidimensional space, into clusters based on a chosen measure of similarity or distance 
\cite{Jain:1999:DCR:331499.331504}. There is no universally agreed upon definition of a 
cluster. Most researchers describe it by considering the internal homogeneity and 
external separation \cite{1427769}.

Intuitively patterns in the same cluster should be more similar to each other than to patterns
in other clusters. There are many clustering algorithms that solve different problems. These 
include partitioning, \textcolor{red}{[TODO]}.

We support two partitioning methods: K-Means and DPMeans. The first is a well studied method
which, given $k$ - the desired number of clusters, partitions the dataset into $k$ clusters by 
minimizing the distance between data points and the nearest centroid \cite{kmeans}. It works
by, initially selecting $k$ random centroids, then in each iteration it first assigns all the 
feature vectors to their nearest centroid constructing a Voronoi diagram regions are associated
with centroids $C = \left\{c_1, c_2, ..., c_k\right\}$. More formally a region of the diagram
is defined as $R(c_i) = \left\{x \in \mathbb{R}^d | d(c_i,x) \le d(c_j,x) \: \forall j\right\}$.
The method then recomputes each centroid $c_i$ as the mean value of all the data points that lie
in region $R(c_i)$:

\begin{equation}
	\label{eq:kmeans-centroids}
	\nonumber
	c_i = \frac{1}{\|R_i\|}\sum\limits_{p_j \in R(c_i)}p_j.
\end{equation}

The procedure terminates when the partitions are the same in two consecutive iterations.

The second method, DP-means, proposed by \cite{DBLP:journals/corr/abs-1111-0352} behaves
similarly to K-Means with the exception that a new cluster is formed whenever a feature
vector is further than $\lambda$ away from every existing centroid. Thus, the number of 
clusters on the output is not known in advance, but is controlled by $\lambda$.

When identifying states, we first remove timestamps from the joined data stream, resulting 
in feature vectors of the form 

\begin{equation}
	\nonumber
	x_i = \left(x_i^{(1)}, x_i^{(2)}, ..., x_i^{(d)}\right).
\end{equation}

These are then clustered using one of the above methods, resulting in a set
of $k$ partitions which are used as lowest level states. When StreamStory \textcolor{red}{[TODO sees]}
a new data point it will assign it to the state (partition) with the nearest centroid.
\begin{equation}
	\label{eq:low-level-assignment}
	\nonumber
	S(p_i) = \argmin_{p_j \in R(c_j)} d(p_i, c_j)
\end{equation}
Where $d$ represents the Euclidean distance.

\subsection{State Aggregation}

Once the lowest level states are computed, StreamStory aggregates them into a state hierarchy.
Before grouping states, we need to measure their relative distance to each other. Since StreamStory
was developed to work with dense dataset, we use the Euclidean distance. Many hierarchical
clustering techniques have been proposed in the literature. They can be characterized
as greedy in the algorithmic sense \cite{DBLP:journals/corr/abs-1105-0121}. Assuming that a pair
of states is merged or agglomerated at each step, the techniques construct a binary tree
commonly known as a dendrogram. This produces a set of states at each level - or each threshold 
value which produces a new partition. Hierarchical clustering methods generally fall into two
groups \cite{Maimon:2005:DMK:1088958}: 
\begin{list}{-}{}
	\item \textbf{Agglomerative} - each object initially represents a cluster of its own. Clusters are
	then iteratively merged, until the desired structure is obtained.
	\item \textbf{Divisive} - when the procedure starts, all object belong to the same cluster. Then
	clusters are recursively divided into sub-clusters until the desired structure is obtained.
\end{list}
These groups can be further subdivided according to the manner that the distance measure is
calculated:
\begin{list}{-}{}
	\item \textbf{Single-link} - methods that consider the distance between two clusters equal to the
	minimum distance of any member of one cluster to any member of the other.
	\item \textbf{Complete-link} - methods that consider the distance between two clusters equal to
	the maximum distance of any member of one cluster to any member of the other.
	\item \textbf{Complete-line} - methods that consider the distance between two clusters equal to
	the average distance of any member of one cluster to any member of the other.
\end{list}
StreamStory uses an agglomerative clustering technique and supports all three of the above
mentioned linkage strategies.

\subsection{Modeling Transitions}

% TODO theory and implementation razlicna
% teorija
% nato implementacija
% zvezni proces v evklidskem prostory
% how to do data mining
% nikoli ne recoveramo zveznega procesa (remark)

We model transitions using a Markovian model. The main characteristic of Markovian models
is that they retain no memory of where they have been in the past. This means that only the
current state can influence where the process will go next. In this work, we are interested
only in processes that can assume a finite set of states, called Markov chains \cite{norris1998markov}.

More formally, we are interested in Markov chains $(X_k)_{k \ge 0}$, which can assume 
values in a finite state space $S$. We define $p_{ij}(k) = P(X_k = j | X_0 = i)$ to be the probability
of the process being in state $j$ at time $k$ when starting from state $i$ at time $0$.
Because of the memoryless property $P(X_k = j | X_0 = i) = P(X_{k+m} = j | X_m = i) = p_{ij}(k)$.

We define a stochastic matrix $P(t)$, where $\left(P(t)\right)_{ij} = p_{ij}(t)$. Thus the
$i$-th row of $P(t)$ is a probability distribution over the state space at time $t$
when the process starts from state $i$. We assume that $P(t)$ is recurrent and define 
$\pi_j = \lim\limits_{t \rightarrow \infty} p_{ij}(t)$. Then $\pi = \left(\pi_1, \pi_2, ..., \pi_m\right)$
is the stationary distribution of $\left(X_t\right)_{t \ge 0}$ and represents the proportion
of time the process spends in each state after running for an infinite amount of time.

Let us now observe the elements if $P(t)$. Since each row of $P(t)$ is a probability
distribution it is clear that $p_{ij}(t) \ge 0$ and $\sum_{j \in S}p_{ij}(t) = 1$.
Furthermore $p_{ij}(t)$ must satisfy the memoryless property $P(X_{t+s} = j | X_s = i) = p_{ij}(t)$.

\subsubsection{State Aggregation}

When observing the process on multiple resolutions we need a way to aggregate states
in $S$. When aggregating states of the Markov chain, we choose a formula which preserves
the stationary distribution. Suppose $A$ and $B$ are non-intersecting subsets of $S$.
Then we can calculate the probability of the process going from set $A$ to set $B$ for any
$t$ as follows:
\begin{equation}
	\label{eq:markov-aggregation}
	p_{AB}(t) = \frac{\sum\pi_i\sum_{j \in A}p_{ij}(t)}{\sum_{i \in B}\pi_i}
\end{equation}



\begin{comment}


What makes these processes useful is that not only do they model many phenomena of interest,
but also the lack of memory property makes it possible to predict how a Markov chain may
behave, and to compute probabilities and expected values which quantify that behavior
\cite{norris1998markov}.
We define a Markov chain as a memoryless stochastic process $\left(X_k\right)_{k \ge 0}$
that can assume a finite set of states $i \in I$, where $I$ is called the state space.


Discrete time Markov chains move in discrete time steps. They are defined using a 
stochastic matrix $P$ where $(P)_{ij} = p_{ij}$ is the probability of jumping from state
$i \in I$ to state $j \in I$ in a single time step. More formally we say that a stochastic
process $\left(X_n\right)_{n \ge 0}$ is a discrete time Markov chain with initial distribution
$\lambda$ if
\begin{enumerate}
	\item $X_0$ has distribution $\lambda$ and
	\item $P(X_{n+1} = i_{n+1} | X_0 = i_0, X_1 = i_1, ..., X_n = i_n) = p_{i_ni_{n+1}}$
\end{enumerate}
Once of the interesting properties of Markov chains are their stationary distributions. A 
stationary distribution describes the proportion of time the process spends in each state.
In our visualization it represents the size of states. The stationary distribution can be
computed as the left eigenvector of $P$ with the corresponding eigenvalue 1.

When drawing the hierarchy we need some way of aggregating states in the Markov chain. 
Suppose we have two non-intersecting sets of states $A = \left\{j_1, j_2, ..., j_k\right\}$ and
$B = \left\{i_1, i_2, ..., i_h\right\}$. Then we use the following formula to compute
the transition probability from set $A$ to set $B$:
\begin{equation}
	\label{eq:dt-aggregation}
	\begin{split}
		p_{AB} = 
		P(X_{n+1} \in A | X_n \in B) = 
		\sum\limits_{j \in A}P(X_{n+1} = j | X_n \in B) = \\
		\sum_{j \in A}\frac{P(X_{n+1} = j)P(X_n \in B | X_{n+1} = j)}{P(X_n \in B)} = 
		\frac{\sum_{j \in A} \pi_j \sum_{i \in B}\frac{p_{ij}\pi_i}{\pi_j}}{\sum_{j \in B} \pi_i} = \\
		\frac{\sum_{i \in B}\pi_i\sum_{j \in A}p_{ij}}{\sum_{j \in B} \pi_i}
	\end{split}
\end{equation}

A natural way of interpolating discrete time steps $\left(p^n\right)_{n \in \mathbb{N}}$ is by the
exponential function $\left(e^{tq}\right)_{t \in \mathbb{R}^{+}}$. Consider a finite state set $I$ and a stochastic
matrix $P$. For any matrix $Q = (q_ij)_{i,j \in I}$, the series
\begin{equation}
	\sum_{k=0}^{\infty}\frac{Q^k}{k!}
\end{equation}
converges componentwise to $e^Q$. Now suppose that we can find a matrix $Q$ such that $e^Q = P$. Then
$e^{nQ} = (e^Q)^n = P^n$, so $\left(e^{tq}\right)_{t \in \mathbb{R}^{+}}$ fills the gaps in the 
discrete sequence.

If we now set $P(t) = e^{tQ}$, then $P(t)$ has the following properties:
\begin{enumerate}
	\item $P(s+t) = P(s)P(t)$
	\item $P(t)$ is the unique solution to equations $P'(t) = P(t)Q$ and $P'(t) = QP(t)$ with
	initial condition $P(0) = I$.
\end{enumerate}
\end{comment}

\subsection{Visualization}

[TODO]

\section{Visualization and User Interaction}

When interacting with the StreamStory system, the user is presented with a two panel user interface
shown in figure \ref{fig:ui-initial}.
 

\begin{figure}[h!]
	\centering
	\makebox[\textwidth]{\includegraphics[width=\textwidth]{ui-initial}}
	\caption{User interface of the StreamStory system.}
	\label{fig:ui-initial}
\end{figure}

The visualization panel on the left visualizes the hierarchical Markovian model. States are 
represented by circles, while transitions are represented by arrows. The size of a state
is proportional to the fraction of time the monitored process spends in the state. When 
StreamStory is used in online mode, the current state is colored green, the most likely
future states blue and the previous state has a red border.

The thickness of an arrow is proportional to the probability of the corresponding transition,
and the most likely transitions have a darker color.

When first opening user interface the user is presented with a top-level chain with only two 
states. They can then use the scroll function to zoom into the hierarchy and the states get 
automatically expanded.

By clicking on a state, the state becomes selected and its details are shown in the "Details"
panel on the right side of figure \ref{fig:ui-initial}. The details panel allows the user
to name the state which, we believe, increases the models interpretability. It also
shows the distribution of all the parameters inside the state as well as the mean value
of each parameter in the state. The mean value is highlighted red or green depending on
how specific the value is for that state. If the value is gray it means that the
value of the attribute is normal compared to the value of the same attribute in other 
states. However, if the value is green or red, it indicates the value is the value is
specific for this state compared to other states.

This is achieved by classifying the instances of the selected state against instances off
all the other states on the same level using a logistic regression model [TODO ref] and 
extracting weights. Values highlighted green indicate a positive weight, while values
highlighted red indicate a negative weight.




















\begin{comment}
\newpage

We propose a suboptimal distribution to be used by each node, which is
easy to compute and does not depend on the number of competing
nodes. A natural candidate is an increasing geometric sequence, in
which
% Numbered Equation
\begin{equation}
\label{eqn:01}
P(t)=\frac{b^{\frac{t+1}{T+1}}-b^{\frac{t}{T+1}}}{b-1},
\end{equation}
where $t=0,{\ldots}\,,T$, and $b$ is a number greater than $1$.

In our algorithm, we use the suboptimal approach for simplicity and
generality. We need to make the distribution of the selected back-off
time slice at each node conform to what is shown in Equation
(\ref{eqn:01}). It is implemented as follows: First, a random
variable $\alpha$ with a uniform distribution within the interval
$(0, 1)$ is generated on each node, then time slice $i$ is selected
according to the following equation:
% Unnumbered Equation
\[
i=\lfloor(T+1)\log_b[\alpha(b-1)+1]\rfloor.
\]
It can be easily proven that the distribution of $i$ conforms to Equation
(\ref{eqn:01}).

So protocols [Bahl 2002,Culler 2001,Zhou 2006,Adya 2001,Culler 2001;
Tzamaloukas-01; Akyildiz-01] that use RTS/CTS
controls\footnote{RTS/CTS controls are required to be implemented by
802.11-compliant devices. They can be used as an optional mechanism
to avoid Hidden Terminal Problems in the 802.11 standard and
protocols based on those similar to [Akyildiz 2001] and
[Adya 2001].} for frequency negotiation and reservation are not
suitable for WSN applications, even though they exhibit good
performance in general wireless ad hoc
networks.

% Head 3
\subsubsection{Exclusive Frequency Assignment}

In exclusive frequency assignment, nodes first exchange their IDs
among two communication hops so that each node knows its two-hop
neighbors' IDs. In the second broadcast, each node beacons all
neighbors' IDs it has collected during the first broadcast period.

% Head 4
\paragraph{Eavesdropping}

Even though the even selection scheme leads to even sharing of
available frequencies among any two-hop neighborhood, it involves a
number of two-hop broadcasts. To reduce the communication cost, we
propose a lightweight eavesdropping scheme.

\subsection{Basic Notations}

As Algorithm~\ref{alg:one} states, for each frequency
number, each node calculates a random number (${\textit{Rnd}}_{\alpha}$) for
itself and a random number (${\textit{Rnd}}_{\beta}$) for each of its two-hop
neighbors with the same pseudorandom number generator.
% Algorithm
\begin{algorithm}[t]
\SetAlgoNoLine
\KwIn{Node $\alpha$'s ID ($ID_{\alpha}$), and node $\alpha$'s
neighbors' IDs within two communication hops.}
\KwOut{The frequency number ($FreNum_{\alpha}$) node $\alpha$ gets assigned.}
$index$ = 0; $FreNum_{\alpha}$ = -1\;
\Repeat{$FreNum_{\alpha} > -1$}{
        $Rnd_{\alpha}$ = Random($ID_{\alpha}$, $index$)\;
        $Found$ = $TRUE$\;
        \For{each node $\beta$ in $\alpha$'s two communication hops
    }{
      $Rnd_{\beta}$ = Random($ID_{\beta}$, $index$)\;
      \If{($Rnd_{\alpha} < Rnd_{\beta}$) \text{or} ($Rnd_{\alpha}$ ==
          $Rnd_{\beta}$ \text{and} $ID_{\alpha} < ID_{\beta}$)\;
      }{
        $Found$ = $FALSE$; break\;
      }
        }
     \eIf{$Found$}{
           $FreNum_{\alpha}$ = $index$\;
         }{
           $index$ ++\;
     }
      }
\caption{Frequency Number Computation}
\label{alg:one}
\end{algorithm}

Bus masters are divided into two disjoint sets, $\mathcal{M}_{RT}$
and $\mathcal{M}_{NRT}$.
% description
\begin{description}
\item[RT Masters]
$\mathcal{M}_{RT}=\{ \vec{m}_{1},\dots,\vec{m}_{n}\}$ denotes the
$n$ RT masters issuing real-time constrained requests. To model the
current request issued by an $\vec{m}_{i}$ in $\mathcal{M}_{RT}$,
three parameters---the recurrence time $(r_i)$, the service cycle
$(c_i)$, and the relative deadline $(d_i)$---are used, with their
relationships.
\item[NRT Masters]
$\mathcal{M}_{NRT}=\{ \vec{m}_{n+1},\dots,\vec{m}_{n+m}\}$ is a set
of $m$ masters issuing nonreal-time constrained requests. In our
model, each $\vec{m}_{j}$ in $\mathcal{M}_{NRT}$ needs only one
parameter, the service cycle, to model the current request it
issues.
\end{description}

Here, a question may arise, since each node has a global ID. Why
don't we just map nodes' IDs within two hops into a group of
frequency numbers and assign those numbers to all nodes within two
hops?

\section{Simulator}
\label{sec:sim}

If the model checker requests successors of a state which are not
created yet, the state space uses the simulator to create the
successors on-the-fly. To create successor states the simulator
conducts the following steps.
% enumerate
\begin{enumerate}
\item Load state into microcontroller model.
\item Determine assignments needed for resolving nondeterminism.
\item For each assignment.
      \begin{enumerate}
      \item either call interrupt handler or simulate effect of next instruction, or
      \item evaluate truth values of atomic propositions.
      \end{enumerate}
\item Return resulting states.
\end{enumerate}
Figure~\ref{fig:one} shows a typical microcontroller C program that
controls an automotive power window lift. The program is one of the
programs used in the case study described in Section~\ref{sec:sim}.
At first sight, the programs looks like an ANSI~C program. It
contains function calls, assignments, if clauses, and while loops.
% Figure
\begin{figure}
\centerline{\includegraphics{acmsmall-mouse}}
\caption{Code before preprocessing.}
\label{fig:one}
\end{figure}

\subsection{Problem Formulation}

The objective of variable coalescence-based offset assignment is to find
both the coalescence scheme and the MWPC on the coalesced graph. We start
with a few definitions and lemmas for variable coalescence.

% Enunciations
\begin{definition}[Coalesced Node (C-Node)]A C-node is a set of
live ranges (webs) in the AG or IG that are coalesced. Nodes within the same
C-node cannot interfere with each other on the IG. Before any coalescing is
done, each live range is a C-node by itself.
\end{definition}

\begin{definition}[C-AG (Coalesced Access Graph)]The C-AG is the access
graph after node coalescence, which is composed of all C-nodes and C-edges.
\end{definition}

\begin{lemma}
The C-MWPC problem is NP-complete.
\end{lemma}
\begin{proof} C-MWPC can be easily reduced to the MWPC problem assuming a
coalescence graph without any edge or a fully connected interference graph.
Therefore, each C-node is an uncoalesced live range after value separation
and C-PC is equivalent to PC. A fully connected interference graph is made
possible when all live ranges interfere with each other. Thus, the C-MWPC
problem is NP-complete.
\end{proof}

\begin{lemma}[Lemma Subhead]The solution to the C-MWPC problem is no
worse than the solution to the MWPC.
\end{lemma}
\begin{proof}
Simply, any solution to the MWPC is also a solution to the
C-MWPC. But some solutions to C-MWPC may not apply to the MWPC (if any
coalescing were made).
\end{proof}

\section{Performance Evaluation}

During all the experiments, the Geographic Forwarding (GF)
[Akyildiz 2001] routing protocol is used. GF exploits geographic
information of nodes and conducts local data-forwarding to achieve
end-to-end routing. Our simulation is
configured according to the settings in
Table~\ref{tab:one}. Each run lasts for 2 minutes and
repeated 100 times. For each data value we present in the results,
we also give its 90\% confidence interval.
% Table
\begin{table}%
\tbl{Simulation Configuration\label{tab:one}}{%
\begin{tabular}{|l|l|}
\hline
TERRAIN{$^a$}   & (200m$\times$200m) Square\\\hline
Node Number     & 289\\\hline
Node Placement  & Uniform\\\hline
Application     & Many-to-Many/Gossip CBR Streams\\\hline
Payload Size    & 32 bytes\\\hline
Routing Layer   & GF\\\hline
MAC Layer       & CSMA/MMSN\\\hline
Radio Layer     & RADIO-ACCNOISE\\\hline
Radio Bandwidth & 250Kbps\\\hline
Radio Range     & 20m--45m\\\hline
\end{tabular}}
\begin{tabnote}%
\Note{Source:}{This is a table
sourcenote. This is a table sourcenote. This is a table
sourcenote.}
\vskip2pt
\Note{Note:}{This is a table footnote.}
\tabnoteentry{$^a$}{This is a table footnote. This is a
table footnote. This is a table footnote.}
\end{tabnote}%
\end{table}%

\section{Conclusions}

In this article, we develop the first multifrequency MAC protocol for
WSN applications in which each device adopts a
single radio transceiver. The different MAC design requirements for
WSNs and general wireless ad-hoc networks are
compared, and a complete WSN multifrequency MAC design (MMSN) is
put forth. During the MMSN design, we analyze and evaluate different
choices for frequency assignments and also discuss the nonuniform
back-off algorithms for the slotted media access design.

% Start of "Sample References" section

\end{comment}

\section{Typical references in new ACM Reference Format}
A paginated journal article \cite{Abril07}, an enumerated
journal article \cite{Cohen07}, a reference to an entire issue \cite{JCohen96},
a monograph (whole book) \cite{Kosiur01}, a monograph/whole book in a series (see 2a in spec. document)
\cite{Harel79}, a divisible-book such as an anthology or compilation \cite{Editor00}
followed by the same example, however we only output the series if the volume number is given
\cite{Editor00a} (so Editor00a's series should NOT be present since it has no vol. no.),
a chapter in a divisible book \cite{Spector90}, a chapter in a divisible book
in a series \cite{Douglass98}, a multi-volume work as book \cite{Knuth97},
an article in a proceedings (of a conference, symposium, workshop for example)
(paginated proceedings article) \cite{Andler79}, a proceedings article
with all possible elements \cite{Smith10}, an example of an enumerated
proceedings article \cite{VanGundy07},
an informally published work \cite{Harel78}, a doctoral dissertation \cite{Clarkson85},
a master's thesis: \cite{anisi03}, an online document / world wide web resource \cite{Thornburg01}, \cite{Ablamowicz07},
\cite{Poker06}, a video game (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03}
and \cite{Lee05} and (Case 3) a patent \cite{JoeScientist001},
work accepted for publication \cite{rous08}, 'YYYYb'-test for prolific author
\cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might contain
'duplicate' DOI and URLs (some SIAM articles) \cite{Kirschmer:2010:AEI:1958016.1958018}.
Boris / Barbara Beeton: multi-volume works as books
\cite{MR781536} and \cite{MR781537}.

% Appendix
\appendix
\section*{APPENDIX}
\setcounter{section}{1}
In this appendix, we measure
the channel switching time of Micaz [CROSSBOW] sensor devices.
In our experiments, one mote alternatingly switches between Channels
11 and 12. Every time after the node switches to a channel, it sends
out a packet immediately and then changes to a new channel as soon
as the transmission is finished. We measure the
number of packets the test mote can send in 10 seconds, denoted as
$N_{1}$. In contrast, we also measure the same value of the test
mote without switching channels, denoted as $N_{2}$. We calculate
the channel-switching time $s$ as
\begin{eqnarray}%
s=\frac{10}{N_{1}}-\frac{10}{N_{2}}. \nonumber
\end{eqnarray}%
By repeating the experiments 100 times, we get the average
channel-switching time of Micaz motes: 24.3$\mu$s.

\appendixhead{ZHOU}

% Acknowledgments
\begin{acks}
The authors would like to thank Dr. Maura Turolla of Telecom
Italia for providing specifications about the application scenario.
\end{acks}

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{acmsmall-sample-bibfile}
                             % Sample .bib file with references that match those in
                             % the 'Specifications Document (V1.5)' as well containing
                             % 'legacy' bibs and bibs with 'alternate codings'.
                             % Gerry Murray - March 2012

% History dates
\received{February 2007}{March 2009}{June 2009}

% Electronic Appendix
\elecappendix

\medskip

\section{This is an example of Appendix section head}

Channel-switching time is measured as the time length it takes for
motes to successfully switch from one channel to another. This
parameter impacts the maximum network throughput, because motes
cannot receive or send any packet during this period of time, and it
also affects the efficiency of toggle snooping in MMSN, where motes
need to sense through channels rapidly.

By repeating experiments 100 times, we get the average
channel-switching time of Micaz motes: 24.3 $\mu$s. We then conduct
the same experiments with different Micaz motes, as well as
experiments with the transmitter switching from Channel 11 to other
channels. In both scenarios, the channel-switching time does not have
obvious changes. (In our experiments, all values are in the range of
23.6 $\mu$s to 24.9 $\mu$s.)

\section{Appendix section head}

The primary consumer of energy in WSNs is idle listening. The key to
reduce idle listening is executing low duty-cycle on nodes. Two
primary approaches are considered in controlling duty-cycles in the
MAC layer.

\end{document}
% End of v2-acmsmall-sample.tex (March 2012) - Gerry Murray, ACM


