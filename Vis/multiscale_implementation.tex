This section presents the implementation of our three step methodology presented in Section \ref{sec:multiscale-framework}.
We start by describing the methods behind the initial state construction in Subsection \ref{sec:state-construction-impl}. \lstopar{[TODO]}

\subsection{Initial State Construction}
\label{sec:state-construction-impl}

As described in section \ref{sec:framework-states} the initial state construction is performed by partitioning
the data points and associating each partition with a single state. The literature suggests many partitioning
algorithms, including those that produce a hierarchical partition. The computational complexity of these algorithms
is, however $O(n^2)$, rendering them unsuitable for large datasets.

We therefore, in the first step, create a flat partition of the data space and later aggregate these partitions
to obtain a hierarchical partition.

Following the intuition of Section \ref{sec:framework-states} we consider that two data points are similar if they lie close
to each other in Euclidean space. \lstopar{Other distance measures are also possible but are out of scope of this paper.}
We use either K-Means or DP-means \cite{DBLP:journals/corr/abs-1111-0352} to construct the partition, representing each 
partition as a Voronoi cell around its centroid. When using StreamStory in real time, the current state is thus
selected by assigning the current observation to the nearest centroid using the following formula:
\begin{equation}
	\nonumber
	i = \argmin_{i \in \mathbb{N}_k} \|x - \mu_i\|.
\end{equation}

We note that the number of centroids (in case of K-Means) or the cluster radius (in case of DP-means) should
be chosen based on domain knowledge and the level of detail the user desires. This parameter also effects the 
initial model construction time as demonstrated in Section \ref{sec:implementation}.

\subsection{Modeling Transition Probabilities}
\label{sec:transition-probs-impl}

As described in Section \ref{sec:framework-transitions} we model state transitions using the continuous
time Markov chain framework, first presented in Section \ref{sec:preliminaries}. The transitions are 
modeled on the lowest level states and are then aggregated using the formula presented in Section \ref{sec:framework-aggregation}.

We allow users to select a subset of attributes which are used to model transition rates. Since the
jump process from state $i$ to state $j$ can be characterized as a Poisson process, we can model its
transition rate $q_{ij}$ as a function of these attributes $q_{ij} = q_{ij}(x)$. We do this by first
discretizing the continuous time parameter into a discrete sequence $(0, \epsilon, 2\epsilon, ...)$,
estimating the transition rates as
\begin{equation}
	q_{ij}(x) = \frac{\epsilon}{\tilde{p}_{ij}(x)}
\end{equation}
where $\tilde{p}_{ij}$ is the estimated probability of jumping from state $i$ to state $j$ in time
$\epsilon$.

Suppose the process is in state $i$ at time $t$ and define a random variable $J_i = j \Leftrightarrow X_{t + \epsilon} = j$.
$J_i$ then has a multinomial distribution with parameters $(p_{i1}, p_{i1}, ..., p_{in})$ which can be 
modeled using a nominal logistic regression model \cite{glm-introduction} to estimate $\tilde{p}_{ij}(x)$.
The transition rate matrix is them constructed on-the-fly from a matrix of logistic regression models.

\subsection{Aggregation}
\label{sec:state-aggregation-impl}

With a fine partition already available, our methodology merges partitions to obtain a hierarchical partition.
Following the intuition form Section \ref{sec:framework-states} we consider two partitions similar, if their
centroids lie close to each other in some metric space. \lstopar{We note that the metric space should be
the same as used for constructing initial states.} We then employ agglomerative clustering \cite{Murtagh83}
producing a dendrogram, with a hierarchical partition and an associated scale $s$ which we directly use
as the scale in our methodology.
Agglomerative clustering algorithms take two arguments: a metric, used to calculate the similarity between 
clusters and a linkage criteria which determines the distance between sets of clusters as a function of the
pairwise distances between observations. We performed experiments using three linkage strategies: single-linkage, 
complete-linkage and average linkage. Our experiments showed no significant difference between the linkage strategies.

