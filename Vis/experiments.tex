\subsection{Prediction Evaluation}

This first experiment is intended to test the validity of our model. The model was constructed on simulated data of 
an electric motor and is shown in Figure \ref{fig:example-motor}. 
\begin{figure*}[]
  	\centering
  	\begin{subfigure}{.48\textwidth}
	  	\centering
	  	\includegraphics[width=\columnwidth]{simulation-processed}
  		\caption{\label{fig:simulation-chart}\lstopar{TODO}}
	\end{subfigure}
  	\begin{subfigure}{.48\textwidth}
	  	\centering
	  	\includegraphics[width=\columnwidth]{model-motor-simulated}
  		\caption{\label{fig:simulation-model}\lstopar{TODO}}
	\end{subfigure}
  	\caption{\lstopar{TODO}}
  	\label{fig:example-motor}
\end{figure*}
The simulation starts in the leftmost state of Figure~\ref{fig:simulation-model} with the motor in a stationary state
and the power switch turned off. Then \lstopar{an invisible user} randomly, sampled from an exponential distribution,
toggles the "on" switch. Once the switch is on, as the rotation increases, the model starts moving in the counter clockwise
direction towards the large green state on the right. This state represents the \lstopar{equilibrium} when the temperature
gained through friction equals the temperature lost to the ambient and the \lstopar{signals} become \lstopar{stationary}.
Once the power switch is toggled again, the rotation slowly halts due to friction and the process goes from right to
left in the counter clockwise direction.

To conduct the experiment we generated two dataset: a training set and a test set. Both datasets
contained $200k$ observations. We built a StreamStory model with 20 lowest-level states on the training set using two attributes:
angular velocity and temperature. The training dataset was then replayed through the model and the finest scale states were stored. We then used
the stored states to calculate the transition probabilities and compared them to the models' jump chain $\Pi$. Since a lot of these probabilities were zero, we decided to use only the probabilities that
are non-zero either in the jump chain or in the probabilities calculated from the history. We then
computed the mean absolute error of the non-zero probabilities which resulted in $MAE=0.05$ or $5\%$.

In another experiment we tested the models predictive power. Two StreamStory models were built: (a) one 
model using the same attributes as in the first experiment, while in the second model (b), we used
the logical switch signal to model state transitions. The models were trained on the same dataset
as in the first experiment. Before the process jumped, we extracted the next state probabilities
and used the state with the highest probability as the predicted next state. We then computed
the prediction accuracy as the ratio between the number of correct predictions and the total number
of jumps (total number of predictions). In this experiment model (a) scored $0.845$ while model
(b) scored $0.904$.


\subsection{Weather Data}
\label{sec:experiments-weather}

The example below shows our model generated on monthly rainfall and temperature data
collected over the course of 20 years between 1920 and 1940 in Nottingham England.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\columnwidth]{example-weather}
	\caption{Qualitative representation of temperature and rainfall data collected over the course of 20 years.}
	\label{fig:example-weather}
\end{figure}

The model was generated using the raw rainfall and temperature data, but each feature vector includes
the rainfall and temperature of the previous month. The states on the right hand side represent the 
summer states, while the states on the left represent winter states. The yearly timeline flows in the
counter clockwise direction with the spring states residing on the bottom of the figure and the autumn
states on the top.

Interestingly, in this dataset, the rainfall and temperature are very highly correlated and the auto naming feature
choose high rainfall as the most significant feature of the summer states. This correlation can be seen
from the attribute histogram shown in Figure \ref{fig:histograms-summer}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\columnwidth]{histograms-summer}
	\caption{\lstopar{TODO caption}}
	\label{fig:histograms-summer}
\end{figure}

\subsection{GPS Data}

The second example was created using raw GPS coordinates collected using a smartphone between \textcolor{red}{X} and \textcolor{red}{Y}.
The data represents the everyday movement of a European computer science researcher. Figure \ref{fig:example-geo}
shows our qualitative representation of this data on a high level with 8 state.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\columnwidth]{geo-states}
	\caption{\lstopar{TODO caption}}
	\label{fig:example-geo}
\end{figure}

From the figure, we can see that the system was able to identify the most typical locations of this persons
movements. Most of the time they spend in central EU including two small states on the bottom and right
representing southern Europe and India where they went for vacation. On the European continent, the system
also identified Germany and Sweden where the person frequently attends meetings or stays for a short while
during a connecting flight. The states on the left represent the USA with the largest state representing New
York city where the person spent the 2014 summer and the smaller states representing San Francisco and Austin,
Texas.

\subsection{Traffic Data}

Our next example, shown in Figure \ref{fig:example-traffic}, shows a representation of a traffic counter positioned on the highway ring around Ljubljana.
\lstopar{In this example, we lifted the counter into a three dimensional space, by also including the number of
cars three hours and one day before.}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\columnwidth]{example-traffic}
	\caption{\lstopar{TODO caption}}
	\label{fig:example-traffic}
\end{figure}
On a high level, our system was able to identify what we believe is the typical daily cycle on the highway ring.
The state on the left hand side represents a night state, when the traffic traffic is the lowest. This state
lasts from 10 PM to 5 AM when most people are asleep and not much is happening on the ring. Then, with a high 
probability, the system jumps into the topmost state with an above average traffic count which lasts from 6 AM
to roughly 9 AM. The next state is the midday state on the right with the highest traffic count lasting from 9 AM
to 7 PM. This is exactly the time when the traffic is very dense and most congestions occur. The next transition
is to the bottommost "evening" state with an average traffic count between 300 and 1000 cars per \lstopar{hour},
lasting from 8 PM to approximately 10 PM. 

\subsection{Domain Experts}