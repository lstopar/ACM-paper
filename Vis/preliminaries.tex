\label{sec:preliminaries}
Our framework is aimed at visualizing multivariate time-series. Here, we introduce the tools required in our framework. We assume as input we are given samples of $d$ signals, which we interpret as one multi-dimensional signal:
$$f: x(t)\mapsto \R^d$$
For simplicity, we assume that the signals are continuous and real-valued as well as uniformly sampled. This means that all signals are sampled simultaneously and at constant intervals. These assumptions can be relaxed in practice, with a preprocessing resampling step.  Each sample then consists of a vector of samples from all input signals at time $t$. 
From this point on we  assume that the image of $f$ defines a compact subset of Euclidean space. For our input, we assume a PL-reconstruction based on time-adjacent samples. 
% MOVE TO DISCUSSION: If our system is intrinsically $d$-dimnesional, then by Whitney's embedding theorem\cite{}, if we have $2d+1$ independent signals, we should be able to reconstruct the system.

% MOVE TO EXPERIMENT SECTION: If we do not have enough independent signals, we can always use a time-delay embedding to raise the dimension of our signal using Takens embedding theorem \cite{} and extensions \cite{}.  

% A time-delay embedding is defined as a function:
% $$ f(x(t),d,\tau) = (x(t),x(t+\tau),\ldots,x(t+(d-1)\tau) )$$
% It takes a time series and lifts it to $d$ dimensions by considering samples spaced at $\tau$ intervals. There is considerable amount of work on how to choose $\tau$, but in our case we often (a) have sufficiently many signals (b) almost any value of $\tau$ works. 

Our main modeling tool is the continuous time Markov chain (CTMC). Markov chains are Markov processes with a finite or countable number of states. The main characteristic of a Markov process is that it has no memory, which means that only its current state influences where it goes next \cite{norris1998markov}.

Markov chains are ubiquitous in modeling over a wide range of phenomena and applications. What makes them particularly useful is that the memoryless property makes it computationally efficient to compute probabilities and expected values of future events.

Formally, a Markov chain is a stochastic process $(X_t)_{t \ge 0}$ which can assume 
only a finite or countable number of states $i \in I$. The starting state
is sampled from a probability distribution $\lambda$, called the initial distribution.
We define $p_{ij}(t)$ to be the probability of the process being in state $j$ at time $t$
when starting from state $i$ at time $0$ and a family of stochastic matrices $\{P(t)\}_{t \ge 0}$,
where the $(i,j)$-th element of $P_{ij}(t)$ equals $p_{ij}(t)$.
Each row of $P(t)$ is thus a probability distribution over the state space $I$. In our work we assume $(X_t)_{t \ge 0}$ irreducible, non-explosive and has a finite number of states. \lstopar{we assume that $(X_t)_{t \ge 0}$  is irreducible ... ???}

% \lstopar{
% \begin{itemize}
% 	\item The state space is finite!
% \end{itemize}
%}

The two most common types of Markov chains are discrete time and continuous time.
Discrete time Markov chains are the most basic type of Markov chains, where the process
changes states in discrete time steps $n \in \mathbb{N}$ with probabilities $p_{ij} = p_{ij}(1)$
and obeys the Markov property presented in Definition \ref{thm:markov-property-discrete}.

\begin{defn}
\label{thm:markov-property-discrete}
Let $(X_n)_{n \ge 0}$ be a discrete time Markov chain with initial distribution $\lambda$.
Then conditional on $X_m = i$, $(X_{m + n})_{n \ge 0}$ is also a Markov chain with initial
distribution $\delta_i$ and is independent of $X_0, X_1, ..., X_m$.
\end{defn}

Continuous time Markov chains are a generalization of discrete time Markov chains as they
fill the gap between the discrete time steps and can change states at any given moment.
Continuous time Markov chains are closely related to Poisson processes. State transition \lstopar{transitions???} can be seen as a Poisson arrival process which depends on the current state. 
%
 %Imagine a labyrinth
%of corridors and chambers and associate each chamber with a single state from the state space
%$I$. The corridor between chambers $i$ and $j$
%is shut by a single door that opens for an infinitely small amount of time at the jump times of
%a Poisson process with rate $q_{ij}$. If the person walking through the labyrinth changes
%chambers each time a door opens, they are performing
%a continuous time Markov chain.
%\primoz{the above paragraph should be rewritten or removed}
%
The basic data needed to define a continuous time Markov chain on state space $I$ are
given by a transition rate matrix $Q$ satisfying three conditions:
\begin{enumerate}
	\item $q_{ij} \ge 0$ for all $i \ne j$
	\item $\sum_{j \in I} q_{ij} = 0$
	\item $-\infty < q_{ii} \le 0$ for all $i \in I$.
\end{enumerate}

Each off-diagonal element of $Q$, $q_{ij}$ represents the rate of jumping from state $i$ to state $j$,
while the diagonal elements $q_{ii}$ represent the rate of leaving $i$. The stochastic matrix
$P(t)$ is represented as $P(t) = e^{tQ}$ and can be computed by solving Kolmogorov's equations:
\begin{equation}
	\nonumber
	\frac{d}{dt}P(t) = P(t)Q
\end{equation}
with initial condition $P(0) = I$.
%
Markov chains can be defined in terms of $n(n-1)$ independent Poisson processes
of rates $q_{ij}$. However, the definition below is much more common:%Definition~\ref{def:ctmc} is \lstopar{much more common in the literature}.
\begin{defn}
	\label{def:ctmc}
	Let $(X_t)_{t \ge 0}$ be a right-continuous process assuming values in $I$ and let $Q$
	be a transition rate matrix. Then for all $t, \epsilon \ge 0$, conditional on $X_t = i$,
	$X_{t+\epsilon}$ is independent of $(X_s)_{0 \le s \le t}$ and as $\epsilon \downarrow 0$
	\begin{equation}
		\nonumber
		P(X_{t+\epsilon} = j | X_t = t) = p_{ij}(\epsilon) = \delta_{ij} + q_{ij}\epsilon + o(\epsilon)
	\end{equation}
	where $\delta_{ij}$ is the Dirac delta function \cite{norris1998markov}.
\end{defn}
When working with Markov
chains one is often interested in its long term properties. One such property is the
stationary distribution, $\pi$. One can show, that for irreducible and non-explosive Markov chains,
the stationary distribution always exists~\cite{norris1998markov} and can be computed as the left eigenvector of $Q$ 
corresponding to the eigenvalue $0$:
\begin{equation}
	\nonumber
	\pi Q = 0.
\end{equation}
In our case, the stationary distribution is interesting because by the convergence theorem~\cite{aldous-fill-2014}
each element $\pi_i$ represents the proportion of time the process spends in state $i$:
\begin{equation}
	\nonumber
	\lim\limits_{t \rightarrow \infty} P(X_t = i) = \pi_i, \forall i \in I.
\end{equation}

To go from our piecewise-linear (PL) input signal to a Markov chain, we consider a spatial discretization of the signal.
We define a \emph{partition function} $g:\R^d\rightarrow I$, where $I$ is a finite, discrete set. This set then represents the states of the Markov chain. If we assume our system is measurable on $\R^d$, then $g$ induces a measure on the set $I$, i.e., let $x(t)\in \R^d$ be the state of the system at time $t$. This induces a corresponding  state $a(t) = g(x(t)) \in I$. This function also defines transition probabililties between the different states. Let $a,b\in I$, then the joint distribution is
    $$\mathbb{P}(a(t_1),b(t_2)) = \mathbb{P}\left(x(t_1)\in g^{-1}(a), y(t_2) \in g^{-1}(b)\right), 	 \quad t_1\leq t_2$$
 In general, this is not Markovian for continuous processes\footnote{This is not Markovian, even for Brownian motion if dimension$>1$ }. The Markovian assumption we make in this step represents the fundamental information loss in our approach\footnote{We can construct arbitrarily good approximations, however the processes do not converge in the limit.}.
Formally, this allows us to define a second map from a measure in $\R^d$ to a Markov chain. However, to avoid overcomplicating the notation, we use $g$ for both. 

 A map between partitions $\phi: I_1 \rightarrow I_2$, is well-defined if it maps each cell of $I_1$ to precisely one cell of $I_2$. That is, for all $x\in\R^d$, ${\phi\circ g_1(x) =  g_2(x)}$\lstopar{for some $g_1, g_2$}. If this relationship holds, it follows that this induces a measurable map between $I_1$ and $I_2$. We use this map to construct a map between states of the Markov chains, which we denote $\Phi$.  We note that in our setting, $\Phi$ can be used to define a map between partitions, since there is a one-to-one correspondence between the partition cells and the states of the Markov chain.  
%\primoz{Last sentence, I am not 100 percent sure about. }
% Let $S\subseteq \R^d$, be the support of our input signal. 

% Given a partition of $S$, denoted by  $\cP$, 
% we can construct a Markov chain. Construct a  state in a Markov chain $\cM_i \in \cM$ for each  partition cell $\cP_i$. Let this mapping be denoted by $f:\cP \rightarrow \cM$. We can endow our data with a canonical measure induced from the volume measure on $\R^{d+1}$ \primoz{here we have to be a bit careful about time}. This induces a measure on the partitions $\mu$, which by the  pushforward $f(\mu)$, induces a measure on the states. This represents the empirical stationary distribution on our Markov chain. To define the transition probabilities, we note that it can be defined in terms of the adjacent partition elements. 
% \primoz{the above paragraph should be synchronized with framework and maps....}

% This completely defines a continuous time Markov chain. Here, we would like to note that this step represents the fundamental information loss in our approach. % DISCUSSION: The transition from continuous process to discrete space (without memory) inherently loses information in any dimension larger than 1.  
% %
% The Markov chain we obtain is highly dependent on the choice of partition. In principle, the initial discretization step  results in a  Markov chain with many states corresponding to a fine partition of the space. Our main contribution is to consider the Markov chains over multiple scales. %This corresponds to a \emph{hierarchical partition of space}. From above, we have that any partition induces a Markov chain. 

% If we are given a sequence of increasingly coarse paritions there is a well defined surjective map 
% $\cP(i)\rightarrow\cP(j)$ from finer to coarser. Each such map also induces a well-defined map between Markov chains. \primoz{Can we prove that these are the same as what we do?}

% In practice, we are not given a hierarchical partition of space, but rather using our initial Markov chain, we define a map between Markov chains by merging states. Note that any such sequence does correspond to a hierarchical partition. \primoz{Maybe add a two line proof here}.  In the following section, we describe how we compute the smaller Markov chain given an initial Markov chain and a map. We also describe how we choose which states to merge, but the intuition is to merge states so that as little of the underlying dynamics is lost. 

% In Section~\ref{sec:multiscale-implementation}, we decribe how we constructed the underlying partition and our initial Markov chain.  

% \primoz{This last part still needs work}

% hierarchical partitions of space

% maps between markov chains

