\label{sec:preliminaries}
Our framework is aimed at visualizing multivariate time-series. Here we introduce the tools required in our framework. We assume as input we are given samples of $d$ signals, which we interpret as one multi-dimensional signal:
$$f: x(t)\mapsto \R^d$$
For simplicity, we assume that the signals are continuous and real-valued as well as uniformly sampled. This means that all signals are sampled simultaneously and at constant intervals. These assumptions can be relaxed in practice, with a preprocessing resampling step.  Each sample then consists of a vector of samples from all input signals at time $t$. If our system is intrinsically $d$-dimnesional, then by Whitney's embedding theorem, if we have $2d+1$ independent signals, we should be able to reconstruct the system.

If we do not have enough independent signals, we can always use a time-delay embedding to raise the dimension of our signal using Takens embedding theorem \cite{} and extensions \cite{}.  

A time-delay embedding is defined as a function:
$$ f(x(t),d,\tau) = (x(t),x(t+\tau),\ldots,x(t+(d-1)\tau) )$$
It takes a times series and lifts it to $d$ dimensions by considering samples spaced at $\tau$ intervals. There is considerable amount of work on how to choose $\tau$, but in our case we often (a) have sufficiently many signals (b) almost any value of $\tau$ works. 

From this point on we  assume that the image of $f$ defines a compact subset of Euclidean space. For our input, we assume a PL-reconstruction based on time-adjacent samples. \primoz{(maybe an example here?)}

Our main modelling tool is the continuous time Markov chain (CTMC). Markov processes are a special kind of stochastic processes with the characteristic of  	having no memory. This means that only the current state of the process can influence	where the process goes next \cite{norris1998markov}. In this work we focus on a special kind
	of Markov processes called Markov chains that can assume only a finite or countable
	number of states.

Markov chains can be used to model many phenomena of interest and have been used in many
	applications, including [TODO]. What makes them particularly useful is that the memoryless
	property makes it feasible to predict future developments and compute probabilities and
	expected values to quantify that behavior.
	
	More formally, a Markov chain is a stochastic process $(X_t)_{t \ge 0}$ which we assume has 
	only a finite or countable number of states $i \in I$. The starting state
	is sampled from a probability distribution $\lambda$, called the initial distribution.
	We define $p_{ij}(t)$ to be the probability of the process being in state $j$ at time $t$
	when starting from state $i$ at time $0$ and a stochastic matrix $P(t)$, where $\left(P(i)\right)_{ij} = p_{ij}(t)$.
	Each row of $P(t)$ is thus a probability distribution over the state space $I$. In our
	work we assume that $P(t)$ is recurrent for every $t$ and that the process is non-explosive.
	
	The literature distinguishes between two types of Markov chains: discrete time and continuous time.
	Discrete time Markov chains are the most basic type of Markov chains, where the process
	changes states in discrete time steps $n \in \mathbb{N}$ with probabilities $p_{ij} = p_{ij}(1)$
	and obeys the Markov property presented in Definition \ref{thm:markov-property-discrete}.
	
	\begin{defn}
	\label{thm:markov-property-discrete}
	Let $(X_n)_{n \ge 0}$ be a discrete time Markov chain with initial distribution $\lambda$.
	Then conditional on $X_m = i$, $(X_{m + n})_{n \ge 0}$ is also a Markov chain with initial
	distribution $\delta_i$ and is independent of $X_0, X_1, ..., X_m$.
	\end{defn}
	
	Continuous time Markov chains are a generalization of discrete time Markov chains as they
	gap between the discrete time steps is filled and the process can change states at any given moment.
	Continuous time Markov chains are closely related to Poisson processes. Imagine the state
	space $I$ as a labyrinth of corridors and chambers. The corridor between chambers $i$ and $j$
	is shut by a single door that opens for an infinitely small amount of time at the jump times of
	a Poisson process with rate $q_{ij}$. If the person walking through the labyrinth changes
	chambers each time a door opens, they are performing
	a continuous time Markov chain.

	\primoz{formal defintion and reference} 
	
	The basic data needed to define a continuous time Markov chain on state space $I$ are
	given by a transition rate matrix $Q$ satisfying the following conditions:
	
	\begin{enumerate}
		\item $q_{ij} \ge 0$ for all $i \ne j$
		\item $\sum_{j \in I} q_{ij} = 0$
		\item $-\infty < q_{ii} \le 0$ for all $i \in I$.
	\end{enumerate}
	
	Each off-diagonal element of $Q$, $q_{ij}$ represents the rate of jumping from state $i$ to state $j$,
	while the diagonal elements $q_{ii}$ represent the rate of leaving $i$. The stochastic matrix
	$P(t)$ is represented as $P(t) = e^{tQ}$ and can be computed by solving Kolmogorov's equations:
	\begin{equation}
		\frac{d}{dt}P(t) = P(t)Q
	\end{equation}
	with initial condition $P(0) = I$.


To go from our PL-input signal to a Markov chain, we consider a spatial discretization of the signal.
Let $S\subseteq \R^d$, be the support of our input signal. Given a partition of $S$, denoted by  $\cP$, 
we can construct a Markov chain. Construct a  state in a Markov chain $\cM_i \in \cM$ for each  partition cell $\cP_i$. Let this mapping be denoted by $f:\cP \rightarrow \cM$. We can endow our data with a canonical measure induced from the volume measure on $\R^{d+1}$ \primoz{here we have to be a bit careful about time}. This induces a measure on the partitions $\mu$, which by the  pushforward $f(\mu)$, induces a measure on the states. This represents the empirical stationary distribution on our Markov chain. To define the transition probabilities, we note that it can be defined in terms of the adjacent partition elements. \primoz{there are a few details to put in here} 

This completely defines a continuous time Markov chain. Here, we would like to note that this step represents the fundamental information loss in our approach. The transition from continuous process to discrete space (without memory) inherently loses information in any dimension larger than 1 (see Appendix).  

The Markov chain we obtain is highly dependent on the choice of partition. In principle, the initial discretization step  results in a relatively large Markov chain corresponding to a fine partition of the space. Our main contribution is to consider the Markov chains over multiple scales. This corresponds to a \emph{hierarchical partition of space}. From above, we have that any partition induces a Markov chain. 

If we are given a sequence of increasingly coarse paritions there is a well defined surjective map 
$\cP(i)\rightarrow\cP(j)$ from finer to coarser. Each such map also induces a well-defined map between Markov chains. \primoz{Can we prove that these are the same as what we do?}

In practice, we are not given a hierarchical partition of space, but rather using our initial Markov chain, we define a map between Markov chains by merging states. Note that any such sequence does correspond to a hierarchical partition. \primoz{Maybe add a two line proof here}.  In the following section, we describe how we compute the smaller Markov chain given an initial Markov chain and a map. We also describe how we choose which states to merge, but the intuition is to merge states so that as little of the underlying dynamics is lost. 

In section~\ref{}, we decribe how we constructed the underlying partition and our initial Markov chain.  


% hierarchical partitions of space

% maps between markov chains

